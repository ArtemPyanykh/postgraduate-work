% -*- coding: cp1251 -*- %
\documentclass[12pt, a4paper]{article}
\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}

\usepackage[centertags]{amsmath}
\usepackage{amsthm,amsfonts,amssymb}
\usepackage{indentfirst}
\sloppy
%-------------------------------------------------------------------------------
\frenchspacing
\parindent=0.6cm
\parskip=2pt
\mathsurround=1pt
%
\makeatletter
\renewcommand{\@biblabel}[1]{#1.}
\makeatother
%
\topskip=0mm
\headsep=0mm
\topmargin=0mm
\headheight=0mm
\textwidth=165mm
\textheight=240mm
\oddsidemargin=0mm

\renewcommand{\baselinestretch}{1.3}
%--------------------------------------------------------------------------

\newtheorem{lemma}{\indent Лемма}
\newtheorem{theorem}{\indent Теорема}
\newtheorem{proposition}{Утверждение}
\newtheorem{remark}{Замечание}
\def\proof{{\indent Доказательство.}}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

%%%%%%%%%%   Custom commands   %%%%%%%%%%%%%%
\newcommand{\Tau}{%
\mathrm{T}
}

\newcommand{\states}{%
\left\{H, L\right\}
}

\newcommand{\generalGame}[2]{%
G_{#1}^{m, \beta}\left({#2}\right)%
}

\newcommand{\infiniteGame}[1]{%
G_{\infty}^{m, \beta}\left({#1}\right)%
}

\newcommand{\firstPlayerPayoff}[4]{%
K_{#1}^{m,\beta}\left({#2}, {#3}, {#4}\right)%
}

\newcommand{\infiniteFirstPlayerPayoff}[3]{%
K_{\infty}^{m,\beta}\left({#1}, {#2}, {#3}\right)%
}

\newcommand{\gameValue}[2]{%
V_{#1}^{m,\beta}\left({#2}\right)%
}

\newcommand{\infiniteGameValue}[1]{%
V_{\infty}^{m,\beta}\left({#1}\right)%
}

\newcommand{\symm}[1]{%
\overline{#1}%
}

\newcommand{\symmGameExpression}[2][\infty]{%
G_{#1}^{m, \alpha}\left({#2}\right)%
}

\newcommand{\upperBound}[1]{%
H_{\infty}^{m,\beta}\left({#1}\right)%
}

\newcommand{\fGeneral}[2][1]{%
\sigma_{#1}^{#2}%
}

\newcommand{\pEven}[1][k]{%
p^0_{#1}%
}

\newcommand{\pOdd}[1][k]{%
p^{\beta}_{#1}%
}

\newcommand{\fEven}[1][k]{%
\phi^0_{#1}%
}

\newcommand{\fOdd}[1][k]{%
\phi^{\beta}_{#1}%
}

\newcommand{\fOpt}{\sigma^*}

\newcommand{\lowerBound}[2][\infty]{%
L_{#1}^{m,\beta}\left({#2}\right)%
}
%%%%%%%%%% End custom commands %%%%%%%%%%%%%%

\begin{document}

\noindent УДК 519.832.2
\medskip

{\bfseries
А.\,И. Пьяных\footnote{Факультет ВМК МГУ, асп.,
                       E--mail: artem.pyanykh@gmail.com},
\medskip

МОДЕЛЬ БИРЖЕВЫХ ТОРГОВ С НЕПОЛНОЙ ИНФОРМАЦИЕЙ \\
\indent С ЭЛЕМЕНТАМИ ПЕРЕГОВОРОВ
}

\begin{quotation}{\footnotesize
  Рассматривается модификация дискретной многошаговой модели биржевых торгов.
  Два игрока ведут между собой торги рисковыми ценными бумагами (акциями).
  Один из игроков (инсайдер) знает настоящую цену акции, второй знает только ее вероятностное распределение.
  На каждом шаге торгов игроки делают целочисленные ставки.
  Игрок, предложивший большую ставку, покупает у второго акцию, причем цена сделки определяется как выпуклая комбинация предложенных ставок с некоторым заданным коэффициентом.
  Получено решение игры бесконечной продолжительности.
  \\
  \indent\textit{Ключевые слова}: многошаговые игры, асимметричная информация, повторяющиеся игры с неполной информацией.}
\end{quotation}
\medskip

{\bfseries 1. Введение.}\ Де Мейером и Салей в \cite{demeyer02} была рассмотрена многошаговая модель биржевых торгов однотипными акциями, в которой торги между собой ведут два игрока.
Перед началом торгов случайный ход определяет цену акции на весь период торгов. В состояниях рынка $L$ и $H$ цена акции равна $0$ и $m \in \mathbb{N}$ соответственно.
Выбранная цена сообщается первому игроку и не сообщается второму, при этом второй знает, что первый игрок --- инсайдер.
Оба игрока знают вероятность $p$ высокой цены акции.
На каждом шаге торгов оба игрока одновременно и независимо назначают некоторую цену за акцию.
Игроку могут делать произвольные вещественные ставки, причем игрок предложивший большую цену покупает у второго акцию по названной цене.
Предложенные цены объявляются игрокам в конце каждого хода.
Игроки помнят предложенные цены на всех предыдущих этапах торгов.
Задачей игроков является максимизация стоимости портфеля.
Модель сводится к повторяющейся игре с неполной информацией, как описано в \cite{aumann95-rep}, для которой Де Мейером и Салей были найдены оптимальные стратегии игроков и значение игры.

Позднее Доманским \cite{domansky07} была рассмотрена модификация модели, в которой ставки игроков могли принимать только целые значения. В данной постановке им было получено решение для игры неограниченной продолжительности.
Отметим также работу В. Крепс \cite{kreps09} с явным решением для $n$-шаговых игр при $m \leq 3$ и работу М. Сандомирской и В. Доманского \cite{sandomirskaya12} с решением одношаговой игры при произвольном $m \in \mathbb{N}$.

Рассмотрим следующий механизм формирования цены акции, предложенный в \cite{chatterjee83} Чаттерджи и Самуэльсоном.
Игроки одновременно предлагают цены $i$ и $j$.
При $i > j$ акция продается по цене $\beta i + (1-\beta) j$, где $\beta \in [0, 1]$ --- заданный коэффициент.
В работе \cite{domansky07} фактически $\beta = 1$.
Решение задачи при значении $\beta = 1/2$ получено в работе \cite{pyanykh14}.
В данной работе решение бесконечной игры найдено при произвольном $\beta \in [0, 1]$.

{\bfseries 2. Модель игры.}\ Пусть множество состояний рынка $ S = \states $. На первом шаге случай выбирает $ s \in S $ с вероятностями $ p(H) = p $ и $ p(L) = 1 - p $.
После этого на протяжении $ n \leq \infty $ шагов игроки играют в игру с матрицей $A^{s,\beta}$, где при $\alpha = 1-\beta$ 
\begin{equation*}
  A^{L,\beta}(i, j) = \begin{cases}
    \alpha i + \beta j, &\, i < j, \\
    0, &\, i = j, \\
    -\beta i - \alpha j, &\, i > j,
  \end{cases}
  \qquad
  A^{H,\beta}(i, j) = \begin{cases}
    \alpha i + \beta j - m, &\, i < j, \\
    0, &\, i = j, \\
    m - \beta i - \alpha j, &\, i > j.
  \end{cases}
\end{equation*}

На $ t $-м шаге первый игрок выбирает ставку $ i_t \in I = \{0, 1, \ldots, m\} $, а второй --- ставку $ j_t \in J = \{0, 1, \ldots, m\} $. 
Выигрыш первого игрока в повторяющейся игре равен $ \sum_{t=1}^n a_{i_t j_t}^{s,\beta} $. 
Второму игроку этот выигрыш становится известным только после окончания игры.
На промежуточных шагах он не имеет точной информации о величинах $ a_{i_t j_t}^{s,\beta} $.

Таким образом, стратегией первого игрока является последовательность ходов
$
  \sigma = (\sigma_1, \sigma_2, \ldots, \sigma_t, \ldots),
$
где $\sigma_t: S \times I^{t-1} \rightarrow \Delta(I)$, а $\Delta(I)$ --- множество вероятностных распределений на $I$.
Как и в \cite{domansky07}, мы ограничимся рассмотрением только тех стратегий $\sigma$, которые гарантируют первому игроку на каждом шаге игры неотрицательный выигрыш.
Множество таких стратегий первого игрока обозначим через $\Sigma$.
Аналогично стратегией второго игрока назовем последовательность ходов
$
  \tau = (\tau_1, \tau_2, \ldots, \tau_t, \ldots),
$
где $ \tau_t: I^{t-1} \rightarrow \Delta(J) $.
Множество стратегий второго игрока обозначим через $\Tau$.

При применении первым игроком смешанной стратегий 
$ \sigma = (\sigma_1, \sigma_2, \ldots, \sigma_n) $, где 
$ 
  \sigma_t~=~(\sigma_t^L, \sigma_t^H), \quad
  \sigma^s_t~=~(\sigma^s_{t, 0}, \ldots, \sigma^s_{t, m}) \in \Delta(I),
$ 
а вторым игроком смешанной стратегии 
$ \tau~=~(\tau_1, \tau_2, \ldots, \tau_n) $, где
$ \tau_t~=~(\tau_{t, 0}, \ldots, \tau_{t, m}) \in \Delta(J) $,
выигрыш первого игрока равен
\begin{equation}
  \label{eq:firstPlayerPayoff}
  \firstPlayerPayoff{n}{p}{\sigma}{\tau} = \sum_{t=1}^n
  \left(
    pA^{H,\beta}(\sigma_t^H, \tau_t) + (1 - p)A^{L,\beta}(\sigma_t^L, \tau_t)
  \right),
\end{equation}
где 
$ 
A^{s,\beta}(\sigma^s_t,\tau_t) = 
    \sum_{i \in I}
        \sum_{j \in J}
            \sigma^s_{t, i} \tau_{t, j} A^{s,\beta}(i, j).
$
Полученную игру обозначим через $\generalGame{n}{p}$, а ее значение через $\gameValue{n}{p}$.

Заметим, что $A^{L,\beta}(i,j) = A^{H,\alpha}(m-i,m-j), \, A^{H,\beta}(i,j) = A^{L,\alpha}(m-i,m-j)$.
Определим симметричные по отношению к $\sigma$ и $\tau$ стратегии игроков
$
  \symm{\sigma_t} = (\symm{\sigma^H_t}, \symm{\sigma^L}), \: \symm{\sigma^s_t} = (\sigma^s_{t,m},\ldots,\sigma^s_{t,0}) \in \Delta(I),
  \symm{\tau_t} = (\tau_{t,m},\ldots,\tau_{t,0}) \in \Delta(J)
$
и симметричную игру $\symm{\generalGame{n}{p}} = \symmGameExpression[n]{1-p}$.
Легко видеть, что выигрыши игроков в симметричных играх при использовании симметричных стратегий совпадают.

{\bfseries 3. Оценка сверху выигрыша первого игрока.}\ Рассмотрим чистую стратегию второго игрока $\tau^k, \, k \in J$:
\[
  \tau^k_1 = k, \quad
  \tau^k_t(i_{t-1}, j_{t-1}) = \begin{cases}
    j_{t-1} - 1, & \, i_{t-1} < j_{t-1}, \\
    j_{t-1},     & \, i_{t-1} = j_{t-1}, \\
    j_{t-1} + 1, & \, i_{t-1} > j_{t-1}.
  \end{cases}
\]

По сути эта стратегия представляет собой стратегию подражания инсайдеру.
Доказательство следующего утверждения проводится по индукции.

\begin{proposition}
  \label{proposition:secondPlayerStrategyPayoffs}
  При применении стратегии $\tau^k$ в игре $\generalGame{n}{p}$ второй игрок гарантирует себе проигрыш не более
  \[
    h_n^L(\tau^k) = \sum_{t=0}^{n-1}(k - t - \alpha)^+, \quad
    h_n^H(\tau^k) = \sum_{t=0}^{n-1}(m - k - t - \beta)^+,
  \]
  в состояниях L и H соответственно, где $x^+ = \max(x; 0)$.
\end{proposition}

Очевидно, что значения $h_n^L(\tau^k)$ и $h_n^H(\tau^k)$ монотонны и ограничены сверху по $n$.
Введем следующую функцию:
\begin{multline*}
  \upperBound{p} = \min_{j \in J} \lim_{n \rightarrow \infty}\left(
    ph_n^H(\tau^j) + (1-p)h_n^L(\tau^j)
  \right) = \\
  %
  = \min_{j \in J} \left(
    p (m - j)(m - j + \alpha - \beta) + (1 - p)j(j + \beta - \alpha)
  \right) / 2.
\end{multline*}

При $p \in \left( (k-\alpha)/m, (k+\beta)/m) \right]$ минимум функции достигается при $j = k$.
Таким образом $\upperBound{p}$ является кусочно-линейной функцией, состоящей из $m~+~1$ линейных сегментов, и полностью определяется своими значениями в точках
\begin{gather*}
  \upperBound{(k+\beta)/m} = \left(
  (m - (k + \beta))(k + \beta) + \alpha\beta
  \right) / 2, \, 
  k = \overline{0, m - 1},\\
  \upperBound{0} = \upperBound{1} = 0.
\end{gather*}

Пусть второй игрок при $p \in \left( (k-\alpha)/m, (k+\beta)/m) \right]$ применяет $\tau^k, \, k \in \overline{0, m}$. Обозначим эту стратегию через $\tau^*$. Тогда справедлива следующая

\begin{lemma}
  \label{lemma:upperBound} 
  При использовании вторым игроком стратегии $\tau^*$ в игре $\infiniteGame{p}$, выигрыш первого игрока ограничен сверху функцией $\upperBound{p}$, т.е.
  \[
  \max_{\sigma \in \Sigma} 
    \infiniteFirstPlayerPayoff{p}{\sigma}{\tau^*}
  \leq \upperBound{p}.
  \]
\end{lemma}

{\bfseries 4. Оценка снизу выигрыша первого игрока.}\ Перейдем к описанию стратегии первого игрока, гарантирующей ему выигрыш не менее $\upperBound{p}$ в игре $\infiniteGame{p}$.
Пусть на определенном шаге игрок применяет 
$
\fGeneral{k}~=~\left(\fGeneral{H}, \fGeneral{L}\right)
$, где 
$\fGeneral{H} = \left(\fGeneral[1,k]{H}, \fGeneral[1,k+1]{H}\right)$,
$\fGeneral{L} = \left(\fGeneral[1,k]{L}, \fGeneral[1,k+1]{L}\right)$ и
$\fGeneral[1,i]{s}$ -- вероятность сделать ставку $i$ в состоянии $s \in \states$.
Применяя $\fGeneral{k}$, инсайдер делает ставки $k$ и $k+1$ с некоторыми заданными вероятностями.

Определим следующие параметры: полные вероятности действий $k$ и $k+1$, равные $q_k$ и $q_{k+1}$ соответственно, а также апостериорные вероятности $p(s|i)$ состояния $s$ при условии, что на предыдущем шаге первый игрок сделал ставку $i$.
Тогда вероятности $\fGeneral[1,i]{s}$ можно найти по формуле Байеса
$
\fGeneral[1,i]{s}~=~p(s|i)q_i/p(s), \: i = k, k + 1.
$

\begin{proposition}
  При использовании $\fGeneral{k}$ первый игрок гарантирует себе на первом шаге выигрыш
  \begin{equation}
    \label{eq:lowerPayoffGeneral}
    \firstPlayerPayoff{1}{p}{\fGeneral{k}}{j} =
    \begin{cases}
      mp - \beta k - \alpha j - \beta q_{k+1},      & \, j < k,     \\
      \left(m p(H|k+1) - k - \beta \right) q_{k+1}, & \, j = k,     \\
      \left(k + \beta - m p(H|k) \right) q_k,       & \, j = k + 1, \\
      \alpha k + \beta j - mp + \alpha q_{k+1},     & \, j > k + 1.
    \end{cases}
  \end{equation}
\end{proposition}

Доказательство следует из (\ref{eq:firstPlayerPayoff}) и определения $\fGeneral{k}$.

Построим оптимальную стратегию инсайдера.
Рассмотрим на $[0,1]$ множество $P$ точек вида
\[
\pEven[i] = \frac{i}{m}, \, i = \overline{0, m}, \quad
\pOdd[i] = \frac{i + \beta}{m}, \, i = \overline{0, m - 1}.
\]

Для $p = \pEven$ определим $\fEven$ как действие $\fGeneral{k}$ с параметрами
\begin{equation*}
  p(H|k) = \pOdd[k-1], \quad p(H|k + 1) = \pOdd[k], \quad q_k = \beta, \quad q_{k+1} = \alpha.
\end{equation*}
Пусть также $\fEven[0] = 0$ и $\fEven[m] = m$ (первый игрок использует максиминную стратегию в соответствующей матричной игре).

Аналогично для $p = \pOdd$ определим $\fOdd$ как действие $\fGeneral{k}$ с параметрами
\begin{equation*}
  p(H|k) = \pEven[k], \quad p(H|k + 1) = \pEven[k+1], \quad q_k = \alpha, \quad q_{k+1} = \beta.
\end{equation*}

\begin{proposition}
  При $p \in P$ для значения $\firstPlayerPayoff{1}{p}{\sigma}{j}$ выигрыша первого игрока верны оценки
  \begin{equation}
    \label{eq:lowerBound:inequalities}
    \min_{j \in J}
    \firstPlayerPayoff{1}{\pEven}{\fEven}{j} \geq 0,
    \quad
    \min_{j \in J}
    \firstPlayerPayoff{1}{\pOdd}{\fOdd}{j} \geq \alpha\beta.
  \end{equation}
\end{proposition}
Справедливость утверждения устанавливается подстановкой $\fEven$ и $\fOdd$ в (\ref{eq:lowerPayoffGeneral}).

Заметим, что если $p \in P$, то при применении инсайдером на первом шаге игры $\fEven$ и $\fOdd$, значения апостериорных вероятностей также принадлежат $P$.
Таким образом, можно продолжить применение $\fEven$ и $\fOdd$ на последующих шагах игры, тем самым определив стратегию $\fOpt$ в игре
$\generalGame{n}{p}, \, p \in P, \, n \in \mathbb{N}$.

Пусть $\lowerBound[n]{p}, \, p \in P$ --- гарантированный выигрыш первого игрока в игре $\generalGame{n}{p}$ при применении стратегии $\fOpt$.
В силу рекурсивной структуры игры $\generalGame{n}{p}$ (см. \cite{domansky07}) и неравенств (\ref{eq:lowerBound:inequalities}) для $\lowerBound[n]{p}$ справедливы формулы
\begin{equation}
  \begin{gathered}
    \label{eq:lowerBound:recurrence:function}
    \lowerBound[n]{(k + \beta)/m} = \alpha\beta +
    \alpha\lowerBound[n-1]{k/m} +
    \beta\lowerBound[n-1]{(k + 1)/m},\\
    % 
    \lowerBound[n]{k/m} = \beta\lowerBound[n-1]{(k - 1 + \beta)/m} +
    \alpha\lowerBound[n-1]{(k + \beta)/m},\\
    % 
    \lowerBound[n]{0} = \lowerBound[n]{1} = 0.
  \end{gathered}
\end{equation}

Так как $\lowerBound[n]{p}$ не убывает по $n$ и ограничена сверху, устремив $n$ к бесконечности, получим нижнюю оценку $\lowerBound{p}$ выигрыша первого игрока в игре $\infiniteGame{p}, \, p \in P$.

Введем следующие обозначения:
\[
L_{2k} = \lowerBound{k/m}, \, k \in \overline{0,m}, \:%
L_{2k+1} = \lowerBound{(k+\beta)/m)}, \, k \in \overline{0,m-1}.
\]
Тогда справедливы формулы
\begin{equation}
  \label{eq:lowerRecurrence}
  \begin{gathered}
    L_{2k+1} = \alpha\beta + \alpha L_{2k} + \beta L_{2(k+1)}, \, k \in \overline{0, m-1},\\
    L_{2k} = \beta L_{2k-1} + \alpha L_{2k+1}, \, k \in \overline{1, m-1},\\
    L_0 = L_{2m} = 0.
  \end{gathered}
\end{equation}

Введем $(2m-1)\times(2m-1)$-матрицу $B$ и $(2m-1)$-вектор-столбец $b$
\begin{equation*}
  B =
  \left(
    \begin{array}{cccccccc}
      1      & -\beta  & 0       & 0      & \cdots & 0      & 0       & 0       \\
      -\beta & 1       & -\alpha & 0      & \cdots & 0      & 0       & 0       \\
      0      & -\alpha & 1       & -\beta & \cdots & 0      & 0       & 0       \\
      \hdotsfor{8}                                                              \\
      0      & 0       & 0       & 0      & \cdots & -\beta & 1       & -\alpha \\
      0      & 0       & 0       & 0      & \cdots & 0      & -\alpha & 1 
    \end{array}
  \right),\quad
  %
  b = \left(
    \begin{array}{c}
      \alpha\beta \\
      0           \\
      \alpha\beta \\
      \cdots      \\
      0           \\
      \alpha\beta
    \end{array}
  \right).
\end{equation*}
Тогда (\ref{eq:lowerRecurrence}) перепишем в виде 
$
BL = B, L_0 = L_{2m} = 0,
$
где 
$L = (L_1, L_2, \ldots, L_{2m-1})$.

Системы $ Mx = f $ с трехдиагональной матрицей $ M $, имеющей структуру
\[
M = \left(\begin{array}{ccccccc}
c_1 & b_1 & 0   & \cdots & 0       & 0       & 0       \\
a_2 & c_2 & b_2 & \cdots & 0       & 0       & 0       \\
\hdotsfor{7}                                           \\
0   & 0   & 0   & \cdots & a_{n-1} & c_{n-1} & b_{n-1} \\
0   & 0   & 0   & \cdots & 0       & a_n     & c_n
\end{array}\right),
\]
можно решать методом прогонки, используя следующие формулы для прогоночных коэффициентов и переменных (см. \cite{samarsky89-num}):
\begin{gather*}
  x_i = \gamma_{i+1} x_{i+1} + \delta_{i+1}, \: i = \overline{1,n-1}, \quad
  x_n = \frac{f_n - a_n\delta_n}{c_n + a_n\gamma_n}, \\
  % 
  \gamma_{i+1} = -\frac{b_i}{c_i + a_i\gamma_i}, \: i = \overline{2, n-1}, \quad
  \gamma_2 = -\frac{b_1}{c_1}, \\
  % 
  \delta_{i+1} = \frac{f_i - a_i\delta_i}{c_i + a_i\gamma_i}, \: i = \overline{2, n-1}, \quad
  \delta_2 = \frac{f_1}{c_1}.
\end{gather*}

\begin{proposition}
  \label{proposition:tridiagonal:coefficients}
  Прогоночные коэффициенты для матрицы $B$ даются следующими формулами:
  \begin{multline*}
    \gamma_{2k} = \frac{\beta+k-1}{k}, \,
    \gamma_{2k+1} = \frac{k}{k+\beta}, \\
    \delta_{2k} = \frac{\alpha(k-1+2\beta)}{2}, \,
    \delta_{2k+1} = \frac{k\beta(k-1+2\beta)}{2(k+\beta)}, \quad k \in \overline{1,m-1}.
  \end{multline*}
\end{proposition}
Данное утверждение доказывается по индукции.

Из (\ref{eq:lowerRecurrence}) следует, что $L_{2k-1} = \gamma_{2k}\gamma_{2k+1}L_{2k+1} + \gamma_{2k}\delta_{2k+1} + \delta_{2k}$.
Подстановкой $\upperBound{(k+\beta)/m}$ вместо $L_{2k+1}$ это равенство обращается в тождество. Тем самым доказано

\begin{proposition}
  \label{proposition:lower:recurrence-solution}
  Решение системы (\ref{eq:lowerRecurrence}) дается следующими формулами:
  \begin{gather*}
    L_{2k+1} = ((m - (k + \beta))(k + \beta) + \alpha\beta)/2, \, k = \overline{0, m-1}, \quad
    L_{2k} = k (m-k)/2, \, k = \overline{0,m}.
  \end{gather*}
\end{proposition}

Итак, мы определили функцию $\lowerBound{p}$ при $p \in P$.
Для $p \in [0, 1] \setminus P$ стратегия инсайдера основана на применении выпуклой комбинации стратегий для крайних точек интервала, в котором находится $p$.

\begin{proposition}
  \label{proposition:first:combination:step}
  При $\lambda \in (0, 1), \, \beta \geq 1/2, \, k = \overline{0, m-1}$ для значения $\firstPlayerPayoff{1}{p}{\sigma}{\tau}$ выигрыша первого игрока верны оценки
  \begin{align*}
    \min_{\tau \in \Tau}
      \firstPlayerPayoff{1}{%
        \lambda \pEven + (1-\lambda) \pOdd}{%
        \lambda \fEven + (1-\lambda) \fOdd}{%
        \tau}
      &\geq \alpha \beta (1-\lambda), \\
      %
    \min_{\tau \in \Tau}
      \firstPlayerPayoff{1}{%
        \lambda \pOdd + (1-\lambda) \pEven[k+1]}{%
        \lambda \fOdd + (1-\lambda) \fEven[k+1]}{%
        \tau}
      &\geq \alpha \beta \lambda.
  \end{align*}
\end{proposition}
Утверждение следует из линейности $\firstPlayerPayoff{1}{p}{\sigma}{\tau}$ по $\sigma$ и равенства (\ref{eq:lowerPayoffGeneral}).

\begin{proposition}
  \label{proposition:first:combination:game}
  При $\lambda \in (0, 1), \, \beta \geq 1/2, \, k = \overline{0, m-1}$, для гарантированного выигрыша первого игрока в игре $\infiniteGame{p}$ справедливо
  \begin{align*}
    \lowerBound{\lambda \pEven + (1-\lambda) \pOdd}      & = 
      \lambda \lowerBound{\pEven} + (1-\lambda) \lowerBound{\pOdd}, \\
    \lowerBound{\lambda \pOdd + (1-\lambda) \pEven[k+1]} & =
      \lambda \lowerBound{\pOdd} + (1-\lambda) \lowerBound{\pEven}.
  \end{align*}
\end{proposition}
\begin{proof}
  Пусть $p = \lambda \pOdd + (1-\lambda) \pEven[k+1]$, где $k \in \{0, \ldots, m - 2\}, \, \lambda \in (0, 1)$.
  Обозначим через $\lambda\fOdd[k] + (1-\lambda)\fEven[k+1]$ распределение, при котором первый игрок рандомизирует выбор ставок $k, k+1, k+2$ с параметрами
  \begin{gather*}
    q_k = \lambda\alpha, \, q_{k+1} = \beta, \, q_{k+2} = (1-\lambda)\alpha,\\
    p(H|k) = \pEven, \, p(H|k+1) = \lambda \pEven[k+1] + (1-\lambda) \pOdd, \, p(H|k+2) = \pOdd[k+1].
  \end{gather*}
  
  Тогда по аналогии с (\ref{eq:lowerBound:recurrence:function}) выписывается следующая рекуррентная формула:
  \[
    \lowerBound{p} = \alpha \beta \lambda +
      \lambda \alpha \lowerBound{\pEven} +
      (1-\lambda) \alpha \lowerBound{\pOdd[k+1]} +
      \beta \lowerBound{(1-\lambda) \pOdd + \lambda \pEven[k+1]}.
  \]
  Так как $(1-\lambda)\pOdd + \lambda \pEven[k+1] \in \left( \pOdd, \pEven[k+1] \right)$, то для $\lowerBound{(1-\lambda)\pOdd + \lambda \pEven[k+1]}$ справедливо аналогичное представление.
  Приведя подобные члены, получим
  \begin{multline}
    \label{eq:first:combination:game:2}
    \lowerBound{p} = \frac{1}{1-\beta^2} 
    \left(\alpha\beta\lambda + \alpha\lambda\lowerBound{\pEven} + (1-\lambda)\alpha\lowerBound{\pOdd[k+1]} + \right.\\
    % 
    \left. + \beta
      \left(
        \alpha\beta(1-\lambda) + (1-\lambda)\alpha\lowerBound{\pEven} + \lambda\alpha\lowerBound{\pOdd[k+1]}
      \right)
    \right) = \\
    % 
    = \left(
      (k + 1)(m - k - 1) + \alpha\lambda(2k - m + 2\beta + 1)
    \right)/2 = \\
    = \lambda\lowerBound{\pOdd} + (1-\lambda)\lowerBound{\pEven[k+1]}.
  \end{multline}
  
  Пусть $p = \lambda\pEven + (1-\lambda)\pOdd$, где $k \in \{1, \ldots, m - 1\}, \, \lambda \in (0, 1)$.
  Обозначим через $\lambda\fEven + (1-~\lambda)\fOdd$ распределение, при котором первый игрок рандомизирует выбор ставок $k$ и $k+1$ с параметрами
  \begin{gather*}
    q_k = \lambda\beta + (1-\lambda)(1-\beta), \:
    p(H|k) = \frac{\lambda\beta}{q_k}\pOdd[k-1] +
      \frac{(1-\lambda)(1-\beta)}{q_k} \pEven,\\
    q_{k+1} = \lambda(1-\beta) + (1-\lambda)\beta, \:
    p(H|k+1) = \frac{\lambda(1-\beta)}{q_{k+1}}\pOdd[k] +
      \frac{(1-\lambda)\beta}{q_{k+1}}\pEven[k+1].
  \end{gather*}
  
  Заметим, что $p(H|k) \in \left( \pOdd[k-1], \pEven \right)$, а $p(H|k+1) \in \left( \pOdd, \pEven[k+1] \right)$.
  Тогда с помощью (\ref{eq:first:combination:game:2}) получим
  \begin{multline*}
    \lowerBound{p} = \lambda \beta \lowerBound{\pOdd[k-1]} + (1-\lambda)(1-\beta)\lowerBound{\pEven} + \lambda(1-\beta)\lowerBound{\pOdd} + \\
      + (1-\lambda)\beta\lowerBound{\pEven[k+1]} =
    \lambda \lowerBound{\pEven} + (1-\lambda) \lowerBound{\pOdd}.
  \end{multline*}
  
  При $p \in \left( 0, \pOdd[1] \right)$ и $p \in \left( \pOdd[m-1], 1 \right)$ доказательство проводится аналогично.
\end{proof}

Заметим, что при $\beta < 1/2$ полученные результаты также имеют место, а стратегию $\fOpt$ можно получить, рассмотрев дополнительную игру $\symm{\infiniteGame{p}}$.
Таким образом, мы определили $\lowerBound{p}$ и $\fOpt$ для любых $p \in [0, 1], \, \beta \in [0, 1]$.

\begin{lemma}
  \label{lemma:first:lower}
  При использовании первым игроком стратегии $\fOpt$ в игре $\infiniteGame{p}$, его выигрыш ограничен снизу функцией $\lowerBound{p}$, т.е.
  \[
  \min_{\tau \in \Tau} \infiniteFirstPlayerPayoff{p}{\fOpt}{\tau} \geq \lowerBound{p}.
  \]
\end{lemma}

Отметим, что приведенная стратегия инсайдера для $p \in [0, 1] \setminus P$ принципиально отличается от аналогичной стратегии в \cite{pyanykh14}, которая дает необходимые результаты только при $\beta = 1/2$.

\begin{theorem}
  Игра $\infiniteGame{p}$ имеет значение $\infiniteGameValue{p} = \upperBound{p} = \lowerBound{p}$, при этом $\fOpt$ -- оптимальная стратегия первого игрока, $\tau^*$ -- оптимальная стратегия второго игрока.
\end{theorem}
Доказательство по форме повторяет доказательство аналогичной теоремы в работе \cite{domansky07}.

%
% Список литературы
%
\renewcommand{\refname}{\begin{center}
{\normalsize \rm СПИСОК ЛИТЕРАТУРЫ} \end{center}}
%
\begin{thebibliography}{99}\itemsep=-2pt
  \selectlanguage{english}
  \bibitem{aumann95-rep}%
    Aumann R.\,J., Maschler M.\,B.\,: Repeated games with incomplete information. The MIT Press. 1995.

  \selectlanguage{english}
  \bibitem{chatterjee83}
    Chatterjee K., Samuelson W.\,: Bargaining under Incomplete Information~// Operations Research. 1983. {\bfseries 31}. P.\, 835--851.

  \selectlanguage{english}
  \bibitem{demeyer02}
    De Meyer B., Saley H.\,: On the strategic origin of Brownian motion in finance~// Int J Game Theory. 2002. {\bfseries 31}. P.\, 285--319.
    
  \selectlanguage{english}
  \bibitem{domansky07}%
    Domansky V.\,: Repeated games with asymmetric information and random price fluctuations at finance markets~// Int J Game Theory. 2007. {\bfseries 36}. N~2. P.\, 265--281.

  \selectlanguage{russian}
  \bibitem{kreps09}%
    Крепс В.\,Л.\,: Повторяющиеся игры, моделирующие биржевые торги, и возвратные последовательности~// Изв. РАН. Теория и системы управления. 2009. {\bfseries 4}. С.\, 109--120.

  \selectlanguage{russian}
  \bibitem{sandomirskaya12}
    Сандомирская М.\,С., Доманский В.\,К.\,: Решение одношаговой игры биржевых торгов с неполной информацией~// Математическая Теория Игр и ее Приложения. 2012. {\bfseries 4}. №~1. С.\, 32-54.

  \selectlanguage{russian}
  \bibitem{pyanykh14}%
    Пьяных А.\,И.\,: Об одной модификации модели биржевых торгов с инсайдером~// Математическая теория игр и ее приложения. 2014. {\bfseries 6}. №~4. С.\, 68--84.
    
  \selectlanguage{russian}
  \bibitem{samarsky89-num}%
    Самарский А.\,А., Гулин А.\,В.\,: Численные методы. М.: Наука, 1989.
\end{thebibliography}
% 
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% Character code reference %%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                             %
%     Upper case russian letters (CP1251): АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ  %                                                                       %
%     Lower case russian letters (CP1251): абвгдеёжзийклмнопрстуфхцчшщъыьэюя  %
%                     Upper case letters: ABCDEFGHIJKLMNOPQRSTUVWXYZ          %
%                     Lower case letters: abcdefghijklmnopqrstuvwxyz          %
%                                   Digits: 0123456789                        %
% Square, curly, angle braces, parentheses: [] {} <> ()                       %
%                Backslash, slash, solidus: \ / |                             %
%       Period, interrogative, exclamation: . ? !                             %
%                 Comma, colon, semi-colon: , : ;                             %
%          Underscore, hyphen, equals sign: _ - =                             %
%             Quotes (left, right, double): ` ' "                             %
%     Commercial-at, hash, dollar, percent: @ # $ %                           %
%  Ampersand, asterisk, plus, caret, tilde: & * +   ^                         %
%                                                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
